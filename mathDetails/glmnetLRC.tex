\documentclass{article}

\usepackage[left=1in, top=1in, right=1in, bottom=1in]{geometry}
\usepackage{graphicx, color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{url}
\usepackage{apacite}
\usepackage{float}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{framed}
\usepackage{here}
\usepackage{zi4}
\usepackage{color}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\median}{median}

\renewcommand{\baselinestretch}{1.2}

\begin{document} 

\title{{\tt glmnetLRC}: Lasso and elastic-net logistic regression classification with an arbitrary loss function\\}
\author{Landon Sego, Alexander Venzin}
\date{March 2016}
\maketitle

\section{Introduction}

The {\tt glmnetLRC} package makes it easy to construct a binary classifier from virtually any number of quantitative predictors 
that will assign an example, or observation, to one of two classes.
It extends the {\tt glmnet} package by making it possible to train lasso or elastic-net logistic 
regression classifiers (LRC's) using a customized, discrete loss function to measure the classification error.  
This allows users to assign unique 
loss values to false positive and false negative errors. 

The logistic regression parameter
estimates are obtained by maximizing the elastic-net penalized likelihood function that contains several tuning parameters. These
tuning parameters are estimated by minimizing the expected loss, which is calculated using cross validation.
This approach was originally implemented to automate the
process of determining the curation quality of mass spectrometry samples \cite{Amidan}. 

We present in detail the algorithm used by the {\tt glmnetLRC} package to identify the optimal parameter estimates
for a logistic regression classifier (LRC) with variables selection implemented by an elastic-net.  

\section{The model}

We begin by defining a
number of variables.  Let $i = 1,\ldots,N$ index the observations in a training dataset. 
Let $y_i = 1$ indicate that observation $i$ belongs to category ``1" and $y_i = 0$ indicate
that it belongs to category ``0".  Per the logistic regression model, let  
\begin{align}
%P(y_i = 1) \equiv \pi_i = \pi(\mathbf{x}_i, \boldsymbol{\beta}) = 
P(y_i = 1) \equiv \pi_i = 
\frac{\exp(\beta_0 + \boldsymbol{\beta}_1^T \mathbf{x}_i)}{1+\exp(\beta_0 + \boldsymbol{\beta}_1^T \mathbf{x}_i)}
\end{align}
\noindent where $\mathbf{x}_i = (x_1, \ldots x_K)^T$ is a vector of predictors, or covariates, that
influence $\pi_i$, $\beta_0$ is an intercept, $\boldsymbol{\beta}_1 = (\beta_1, \ldots, \beta_K)^T$ is a 
vector of logistic regression coefficients, and for notational convenience, 
$\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots \beta_K)^T$. 

The estimate of the vector of regression parameters, $\boldsymbol{\beta}$, is influenced by two other tuning parameters, 
$\alpha$ and $\lambda$.  For this reason, we will often write $\boldsymbol\beta$ as $\boldsymbol\beta(\alpha,\lambda)$.  
The value of $\lambda > 0$ controls the weight of the
penalty of the log-likelihood function, while $\alpha$ controls the mixture of the ridge and lasso penalties.  
The relationship between $\boldsymbol{\beta}$, $\alpha$, and $\lambda$ will be clarified below.  A final tuning parameter, 
$\tau \in (0, 1)$, provides a threshold for the LRC such that if $\pi_i > \tau$, observation $i$ is predicted to belong
to class ``1''.

\section{Estimating the regression parameters}

When we fit the elastic-net logistic regression model to the data, we obtain the estimator 
$\hat{\boldsymbol\beta}(\alpha,\lambda)$.  Therefore, 
let $\hat\pi_i \equiv \pi \bigl( \mathbf{x}_i,\hat{\boldsymbol\beta}(\alpha,\lambda) \bigr)$ denote the predicted probability 
that $y_i = 1$.  Then, if $\hat\pi_i > \tau$, the LRC predicts that $y_i = 1$, otherwise it predicts that $y_i = 0$.  
It will be useful to represent the predicted class of observation
$i$ as $\hat{y}_i \equiv f \bigl( \mathbf{x}_i,\hat{\boldsymbol{\beta}}(\alpha,\lambda),\tau \bigr) = 
I_{(\hat\pi_i > \tau)}$, where $f$ can be thought of as the LRC.
For the elastic-net, the estimate $\hat{\boldsymbol{\beta}}$ is the $\boldsymbol{\beta}$
that maximizes the penalized, binomial log-likelihood function:
\begin{align}
\label{eq:penalized_likelihood}
\hat{\boldsymbol{\beta}}(\alpha,\lambda) = \argmax_{\boldsymbol\beta \in \mathbb{R}^{K+1}} \Biggl[ \ell(\mathbf{x}_i,\boldsymbol{\beta}) - \lambda 
\biggl( \frac{1-\alpha}{2} \sum_{k=1}^K \beta_k^2 + \alpha \sum_{k=1}^K |\beta_k| \biggr) \Biggr]
\end{align}
\noindent where the unpenalized log-likelihood is given by
\begin{align}
\label{eq:unpenalized_likelihood}
%\ell(\mathbf{x}_i,\boldsymbol{\beta}) = \sum_{i=1}^N \bigl[ y_i \log(\pi_i) + (1 - y_i)\log(1-\pi_i) \bigr]
\ell(\mathbf{x}_i,\boldsymbol{\beta}) = \frac{1}{N} \sum_{i=1}^N \biggl[ y_i \bigl( \beta_0 + \boldsymbol\beta_1^T\mathbf{x}_i \bigr) 
  - \log \bigl( 1 + \exp(\beta_0 + \boldsymbol\beta_1^T\mathbf{x}_i ) \bigl) \biggr]
\end{align}
\noindent Parenthetically, $\alpha = 1$ is the lasso penalty, $\alpha = 0$ is the ridge regression penalty,
and $0 < \alpha < 1$ is a mixture of the two. The penalty in \eqref{eq:penalized_likelihood} is the one 
specified in the documentation of the {\tt glmnet} package \cite{glmnet}.


\section{Estimating the tuning parameters}

The optimal values of the tuning parameters, $\alpha$, $\lambda$, and $\tau$, are obtained by minimizing 
the risk, or expected loss, of 
the LRC, where the risk is calculated via cross validation.  Calculating risk requires that we define a discrete 
loss function, $L(y,\hat{y})$ as follows:
\begin{table}[H]
\begin{center}
\begin{tabular}{c|cc}
& $\hat{y} = 0$ & $\hat{y} = 1$ \\
\hline
$y = 0$ & $0$ & $\kappa_0$ \\
$y = 1$ & $\kappa_1$ & $0$ \\
\end{tabular}
\end{center}
\end{table}
\noindent with $\kappa_0 > 0$ and $\kappa_1 > 0$ chosen to reflect the severity of false-positive and 
false-negative errors, respectively.  Setting $\kappa_0 = \kappa_1 = 1$ results in the commonly used 0-1 loss
function.  In the {\tt glmnetLRC} package, $L$ is specified via a call to {\tt lossMatrix()}, and the result is passed to
the {\tt lossMat} argument of {\tt glmnetLRC()}.  In fact, all four elements of $L$ can be specified as desired, i.e.,
the diagonal elements are not required to be 0, as suggested above.

Cross validation is accomplished by randomly partitioning the data into $M$ folds (non-overlapping and
exhaustive subsets), where each fold is tested using a model trained on the remaining folds. 
The value of $M$ is controlled by the {\tt cvFolds} argument in 
{\tt glmnetLRC()}. Following the presentation of Hastie, et al. \citeyear{Hastie}, let
\begin{align}
\label{eq:cv_map}
\delta:\{1,\ldots,N\} \rightarrow \{1, \ldots, M\}
\end{align}
\noindent map each observation in the training data to one of the folds.
Let $\hat{\boldsymbol{\beta}}^{-m}(\alpha,\lambda)$ represent the estimate of $\boldsymbol\beta$ obtained by
fitting the elastic-net logistic regression model to all the training data except the $m^{\text{th}}$ fold.
The cross validation estimate of the risk is given by
\begin{align}
\label{eq:risk}
R(\alpha,\lambda,\tau) = \sum_{i=1}^N w_i L \Bigl(y_i, f\bigl(\mathbf{x}_i,
~\hat{\boldsymbol{\beta}}^{-\delta(i)}(\alpha,\lambda), ~\tau \bigr) \Bigr) \bigg/ \sum_{i=1}^N w_i
\end{align}
\noindent where the weights $w_i$ are specified in the {\tt lossWeight} argument to {\tt glmnetLRC()}. 
The optimal estimates of the tuning parameters are those that minimize the risk:
\begin{align}
\label{eq:tuning_estimates}
(\hat\alpha,\hat\lambda,\hat\tau) = \argmin_{\alpha,\lambda,\tau} R(\alpha,\lambda,\tau)
\end{align}

In practice, we calculate $(\hat\alpha$, $\hat\lambda$, $\hat\tau)$ by computing \eqref{eq:risk} over an irregular
cube of discrete parameter values, defined by the combination of three vectors:  
$\boldsymbol\alpha \times \boldsymbol\lambda \times \boldsymbol\tau$.
The point in the cube that minimizes the risk becomes the estimate for $(\alpha, \lambda, \tau)$. In the event there
are ties for the lowest risk for two or more points in the cube, points with $\tau$ nearer to $0.5$ are preferred, and
if that still doesn't break the tie, points with larger values of $\lambda$ are preferred because they result in
a more parsimonious model with fewer predictors.
The values of $\boldsymbol\alpha$ and $\boldsymbol\tau$ are specified by the {\tt alphaVec} and {\tt tauVec} arguments
of {\tt glmnetLRC()}, respectively.  The values of $\boldsymbol\lambda$ depend on each $\alpha$ and are chosen algorithmically
by {\tt glmnet()}, using the default values for the relevant arguments in {\tt glmnet()} and using the entire training dataset.

\section{The final LRC model}

So far in this discussion, we have made reference to a single random partition of the data, $\delta$, into $M$ folds. 
Naturally, the estimates of the tuning parameters depend on $\delta$. A different partition will yield different
estimates of the tuning parameters.  To ensure the final LRC is robust to the random 
partitioning process, we repeat the training process for multiple partitions, $\delta_1,\ldots,\delta_J$, producing
$(\hat\alpha_j, \hat\lambda_j, \hat\tau_j)$ for $j = 1,\ldots,J$.  
The value of $J$ is controlled by the {\tt cvReps} argument in {\tt glmnetLRC()}. We subsequently
refer to the repetition of the cross validation process as cross validation replication.

Calling the {\tt plot()} method
on the object returned by {\tt glmnetLRC()} shows a pairs plot and univariate histogram of the
various $(\hat\alpha_j, \hat\lambda_j, \hat\tau_j)$.  This plot illustrates the consistency (or lack thereof) 
of the tuning parameter estimates across cross validation replicates.

Once $\hat\alpha_j$, $\hat\lambda_j$, and $\hat\tau_j$ are identified for all the cross validation replicates, 
the final estimate of the tuning parameters is obtained by calculating the median of each one separately:
\begin{align}
\label{eq:final_tuning}
(\hat\alpha^\star,\hat\lambda^\star,\hat\tau^\star) = \bigl(\median_j(\hat\alpha_j), ~\median_j(\hat\lambda_j),
  ~\median_j(\hat\tau_j) \bigr)
\end{align}
\noindent The final estimator $\hat{\boldsymbol\beta}(\hat\alpha^\star,\hat\lambda^\star)$ is obtained by fitting all the training 
data (via \eqref{eq:penalized_likelihood}) using the final estimates of the tuning parameters \eqref{eq:final_tuning}, 
which gives rise to the final LRC:
\begin{align}
f^\star \equiv f \bigl(\mathbf{x}, ~\hat{\boldsymbol{\beta}}(\hat\alpha^\star, \hat\lambda^\star), ~\hat\tau^\star \bigr)
\end{align}
\noindent Calling the {\tt predict()} method on the object returned by {\tt glmnetLRC()} uses $f^\star$ to classify
new observations.  Likewise, calling the {\tt coef()} method on the object returned by {\tt glmnetLRC()} returns 
$\hat{\boldsymbol\beta}(\hat\alpha^\star,\hat\lambda^\star)$.

\section{The cross validation estimate of the risk}

A measure of overall performance for the LRC is provided by the cross validation estimate of the risk. 
Using the final tuning parameter estimates $(\hat\alpha^\star,\hat\lambda^\star)$ defined by \eqref{eq:final_tuning}, 
a corresponding set of regression parameter estimates, $\hat{\boldsymbol\beta}_j^{-m}(\hat\alpha^\star,\hat\lambda^\star)$, 
are obtained using \eqref{eq:penalized_likelihood} 
for each of the $M$ folds in replicate $j$.  The estimate of the risk for replicate $j$ is given by applying \eqref{eq:risk} 
as follows:
\begin{align}
\label{eq:final_risk_j}
R_j  = \sum_{i=1}^N w_i L \Bigl(y_i, ~ f\bigl(\mathbf{x}_i,
~\hat{\boldsymbol\beta}_j^{-\delta_j(i)}(\hat\alpha^\star,\hat\lambda^\star),  ~\hat\tau^\star \bigr) \Bigr) \bigg/ \sum_{i=1}^N w_i
\end{align}
The value of $R_j$ is calculated for $j = 1,\ldots,J$ and summarized using the mean and standard deviation
in the usual way:
\begin{align}
\bar{R} = \frac{1}{J}\sum_{j=1}^J R_j,  ~~~~ \sigma_R = \sqrt{\frac{\sum_{j=1}^J(R_j - \bar{R})^2}{J-1}}
\end{align}
The values of $\bar{R}$ and $\sigma_R$ are obtained by calling {\tt glmnetLRC()} with the argument {\tt estimateLoss = TRUE}
and then printing the resulting object.

%\nocite{*}
\bibliographystyle{apacite}
\bibliography{References}


\end{document}
